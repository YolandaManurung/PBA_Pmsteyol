{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from matplotlib import pyplot\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data.csv',\n",
    "                   header=None, skiprows=[0],\n",
    "                   usecols=[0,4,5,6,8,9,13,14]).dropna()\n",
    "df.columns = ['book_title', 'book_image_url', 'book_rating','book_author','genre', 'reviewer_name', 'review', 'ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df['cleaned'] = df['review'].apply(_removeNonAscii)\n",
    "\n",
    "df['cleaned'] = df.cleaned.apply(func = make_lower_case)\n",
    "df['cleaned'] = df.cleaned.apply(func = remove_stop_words)\n",
    "df['cleaned'] = df.cleaned.apply(func=remove_punctuation)\n",
    "df['cleaned'] = df.cleaned.apply(func=remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the description into words\n",
    "\n",
    "corpus = []\n",
    "for words in df['cleaned']:\n",
    "    corpus.append(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building TFIDF model and calculate TFIDF score\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df = 5, stop_words='english')\n",
    "tfidf.fit(df['cleaned'])\n",
    "\n",
    "# Getting the words from the TF-IDF model\n",
    "\n",
    "tfidf_list = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\n",
    "tfidf_feature = tfidf.get_feature_names() # tfidf words/col-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our corpus with Google Pretrained Model\n",
    "google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
    "google_model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building TF-IDF Word2Vec \n",
    "\n",
    "# Storing the TFIDF Word2Vec embeddings\n",
    "tfidf_vectors = []; \n",
    "line = 0;\n",
    "# for each book description\n",
    "for desc in corpus: \n",
    "  # Word vectors are of zero length (Used 300 dimensions)\n",
    "    sent_vec = np.zeros(300) \n",
    "    # num of words with a valid vector in the book description\n",
    "    weight_sum =0; \n",
    "    # for each word in the book description\n",
    "    for word in desc: \n",
    "        if word in google_model.wv.vocab and word in tfidf_feature:\n",
    "            vec = google_model.wv[word]\n",
    "            tf_idf = tfidf_list[word] * (desc.count(word) / len(desc))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_vectors.append(sent_vec)\n",
    "    line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommending top 5 similar books\n",
    "\n",
    "def recommendations(book_title):\n",
    "    \n",
    "    # finding cosine similarity for the vectors\n",
    "\n",
    "    cosine_similarities = cosine_similarity(tfidf_vectors,  tfidf_vectors)\n",
    "    \n",
    "    # taking the title and book image link and store in new data frame called books\n",
    "    books = df[['book_title', 'book_image_url']]\n",
    "    #Reverse mapping of the index\n",
    "    indices = pd.Series(df.index, index = df['book_title']).drop_duplicates()\n",
    "         \n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    recommend = books.iloc[book_indices]\n",
    "    for index, row in recommend.iterrows():\n",
    "\n",
    "        response = requests.get(row['book_image_url'])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.title(row['book_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations('A Court of Wings and Ruin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
